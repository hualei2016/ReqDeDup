{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20337b6c-78e6-4aaf-b65a-2a17ba1758cf",
   "metadata": {},
   "source": [
    "## 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71419e08-42c3-4645-9b47-1b1ae53cffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not need all the imports below, but too lazy to clean up ~\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ['USER_AGENT'] = 'my-req-dedup-agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b33582-bad0-4ab0-ab42-5e526554ca9d",
   "metadata": {},
   "source": [
    "## 1. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7b71d-7202-409e-b857-e171d02920bf",
   "metadata": {},
   "source": [
    "### read env variables from ~/.env, because Jubyter seems not to source bashrc, zshrc or any profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8883a9c9-6130-462e-96af-19bd9c4e37d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:48) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/hualei/Library/Python/3.12/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages']\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()    # this is needed, because we store in ~/.env\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7002ee3-3f24-4fda-937e-e9b1e05f4736",
   "metadata": {},
   "source": [
    "### save an array into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92a328f-3c84-48a4-b731-4311f7455fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array_to_json(array, file_name):\n",
    "\n",
    "    with open(file_name, mode='w') as file:\n",
    "        json.dump(array, file)\n",
    "\n",
    "    print(f\"Save to {file_name} successfully!\\n\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d672d-aa01-44df-8c07-5760b49b90a3",
   "metadata": {},
   "source": [
    "### load an array from a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d54db5-235d-45d4-a534-0d04b0e9b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array_from_json(file_name):\n",
    "    # 从 JSON 文件中读取一维数组\n",
    "    with open(file_name, mode='r') as file:\n",
    "        array = json.load(file)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12efa87-860a-4f43-839f-a84ba6b585cc",
   "metadata": {},
   "source": [
    "### save json data line by line into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338c07b9-b53d-430d-b921-7d5a87de82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件并逐行写入 JSON 数据\n",
    "def save_json_in_lines(json_data_list, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for json_data in json_data_list:\n",
    "            json_line = json.dumps(json_data)  # 将 JSON 对象转换为字符串\n",
    "            file.write(json_line + '\\n')       # 写入文件并换行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b4754-a32a-4e06-afc9-f4f6d6e0d7b0",
   "metadata": {},
   "source": [
    "### load json data line by line to a json array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4982e7-10a1-4259-baa2-68a91be62be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件并逐行读取每个 JSON 对象\n",
    "def load_json_in_lines(file_name):\n",
    "    json_array = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())  # 解析 JSON 并添加到数组中\n",
    "            json_array.append(json_obj)\n",
    "    \n",
    "    print(\"从文件读取的JSON数据：\")\n",
    "\n",
    "    return json_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc160c-0d8e-4b10-827c-91b7a0edeaa9",
   "metadata": {},
   "source": [
    "### save a 2D array into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356cece2-d874-462d-a7b8-ac334124ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_2D_array_to_csv(array, file_name):\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(array)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf25dfd-cc9c-4c75-8194-66d24166908d",
   "metadata": {},
   "source": [
    "### load a 2D float array from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11b5e11-66d1-488e-a9c4-a4c381fa35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2D_float_array_from_csv(file_name):\n",
    "    with open(file_name, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        array = [list(map(floag, row)) for row in reader]\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f8f74-04e4-4b34-bffb-e516d796ee4f",
   "metadata": {},
   "source": [
    "## 2. Calculate the Cosine Similarity, based on embedding of each requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472fcc0-1e03-47a0-acd3-0fe5cde7cbac",
   "metadata": {},
   "source": [
    "### Prepare the embedding model\n",
    "@todo: We may try different models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a1bd996-368c-443e-bb1d-440ec670996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_model():\n",
    "    # Embed\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    return hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8de977-16ea-4bf4-8ec4-c5e5b9619374",
   "metadata": {},
   "source": [
    "### Calculate embedding for each row of a csv file, based on \"subject\" and \"description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecbae602-e678-4f00-abd3-f88e60c9d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embedding_from_csv(csv_file_name, embedding_model):\n",
    "    # Read each record\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "\n",
    "    # 打印前几条需求数据\n",
    "    #print(df.head())\n",
    "\n",
    "    # init the embeddings\n",
    "    req_embeddings = []\n",
    "\n",
    "    # 遍历每一行, 计算embedding\n",
    "    for index, row in df.iterrows():\n",
    "        if (index > 0) and (index % 100 == 0):\n",
    "            print(f\"\\nFinshed calculating embedding for {index} Requirements: \\n\")\n",
    "        \n",
    "        req = row.to_dict()\n",
    "        #print(f'req: {req}')\n",
    "        sub_and_desc = '--Subject--\\n' + str(req['subject']) + '\\n\\n--Description--\\n' + str(req['description'])\n",
    "        #if (index > 0) and (index % 20 == 0):\n",
    "        #    print(f\"\\n ===> sub_and_desc is {sub_and_desc} ===> \\n\")\n",
    "        embed = embedding_model.embed_query(sub_and_desc)\n",
    "        #print(f'Requirement NO.{index+1}: embed: {embed}\\n')\n",
    "        req_embeddings.append(embed)\n",
    "    \n",
    "        #print(f'----- len(req_embeddings) = {len(req_embeddings)} -----\\n')\n",
    "\n",
    "    # 记录需求总数\n",
    "    NUM_OF_REQ = index+1\n",
    "    print(f'======= NUM_OF_REQ = {NUM_OF_REQ} =======\\n')\n",
    "\n",
    "    return req_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d3394-f130-45ba-92a6-b8071ef70eaf",
   "metadata": {},
   "source": [
    "### Calculate correlation coefficient based on every 2 record's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8407fd1-a130-40dc-8833-09483e8e29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr(embeddings_array):\n",
    "    NUM_OF_REQ = len(embeddings_array)\n",
    "\n",
    "    # 初始化相关系数数组\n",
    "    rows, cols = NUM_OF_REQ, NUM_OF_REQ\n",
    "    req_cor = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "    \n",
    "    # 遍历计算每两个需求之间的相似度（点积）\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        if (i > 0) and ((i+1) % 100 == 0):\n",
    "            print(f\"\\nCalculating correlation coefficient for Requirement NO.{i + 1}: \\n\")\n",
    "        req_cor[i][i] = 1\n",
    "        for j in range(i):\n",
    "            req_cor[i][j] = req_cor[j][i] = np.dot(req_embeddings[i], req_embeddings[j])\n",
    "\n",
    "    return req_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbf2d3-454c-488e-af6a-efbd5f6656c6",
   "metadata": {},
   "source": [
    "### Calculate L2 norm for all the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31aa3393-0a32-4d39-ab66-cf38b07d3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l2_norm(embeddings_array):\n",
    "    NUM_OF_REQ = len(embeddings_array)\n",
    "\n",
    "    # 初始化相关系数数组\n",
    "    l2_norm = [0 for _ in range(NUM_OF_REQ)]\n",
    "    \n",
    "    # 遍历计算每两个需求之间的相似度（点积）\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        l2_norm[i] = np.linalg.norm(embeddings_array[i])\n",
    "\n",
    "    return l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ca28f-2f38-442f-b13e-9b0765eb18b2",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f44866-0596-42b4-84ae-839cdb1282b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_consine_similarity(req_cor, l2_norm):\n",
    "    NUM_OF_REQ = len(l2_norm)\n",
    "    assert NUM_OF_REQ == len(req_cor)\n",
    "\n",
    "    consine_similarity = [[0 for _ in range(NUM_OF_REQ)]for _ in range(NUM_OF_REQ)]\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        for j in range(NUM_OF_REQ):\n",
    "            consine_similarity[i][j] = consine_similarity[j][i] = req_cor[i][j]/(l2_norm[i] * l2_norm[j])\n",
    "\n",
    "    return consine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75441a07-1b94-488a-b452-16283d99b0fe",
   "metadata": {},
   "source": [
    "## 3. Use batch API of TongYi, to find whether two requirements are duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274b1d6-18e6-4915-ab2b-ec5d1c0b144b",
   "metadata": {},
   "source": [
    "### Form the prompt string with the two requirements in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3364f4a7-c33a-4c1f-a36b-2a3f20658cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_query(req1, req2):\n",
    "    \n",
    "    query_content = '''\n",
    "    Please compare the following two requirements, with subject and description, and tell me whether they are very similar and should be duplicated.\n",
    "    Please reply with the following format:\n",
    "    * Probability: a number between 0% to 100%, showing how much you recommend to set the two tickets to duplicated\n",
    "    * Analysis: Provide your detailed recommendation\n",
    "    * New Requirement: If the probability is > 70%, draft a new requirement to combine the old two requirements, with\n",
    "    ** Subject: <the new subject>\n",
    "    ** Description: <the new description>\n",
    "    '''\n",
    "    \n",
    "    req1__sub_and_desc = '--Subject--\\n' + str(req1['subject']) + '\\n\\n--Description--\\n' + str(req1['description'])\n",
    "    req2__sub_and_desc = '--Subject--\\n' + str(req2['subject']) + '\\n\\n--Description--\\n' + str(req2['description'])\n",
    "\n",
    "    query_content = query_content + '\\n\\n-- Ticket1\\n' + req1__sub_and_desc + '\\n\\n-- Ticket2\\n' + req2__sub_and_desc\n",
    "\n",
    "    return query_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22299ea-9513-458b-b54d-8531541acb8f",
   "metadata": {},
   "source": [
    "### Parse Response from TongYi, to get 'Probability', 'Analysis', 'New Requirement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afec27d2-76b3-4841-8639-16554df79d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rsp(text):\n",
    "    #pattern = r'\\* Probability:\\s*(.*?)\\n\\* Analysis:\\s*(.*?)\\n\\* New Requirement:\\s*(.*?)\\n'\n",
    "    #matches = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    # 使用正则表达式匹配, 允许匹配多行 (>0.5时，每段前面有'*')\n",
    "    pattern = r'\\* Probability:\\s*(.*?)\\s*\\n\\* Analysis:\\s*([\\s\\S]*?)\\s*\\n\\* New Requirement:\\s*([\\s\\S]*)'\n",
    "    matches = re.search(pattern, text, re.MULTILINE)\n",
    "\n",
    "    if not matches:\n",
    "        # 使用正则表达式匹配, 允许匹配多行 (<=0.5时，每段前面没有'*')\n",
    "        pattern = r'Probability:\\s*(.*?)\\s*\\nAnalysis:\\s*([\\s\\S]*?)\\s*\\nNew Requirement:\\s*([\\s\\S]*)'\n",
    "        matches = re.search(pattern, text, re.MULTILINE)\n",
    "\n",
    "    if matches:\n",
    "        return {\n",
    "            'Probability': matches.group(1).strip(),\n",
    "            'Analysis': matches.group(2).strip(),\n",
    "            'New Requirement': matches.group(3).strip()\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'Probability': None,\n",
    "            'Analysis': None,\n",
    "            'New Requirement': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3deb1c-cb31-463a-ba84-bc30c8cd7087",
   "metadata": {},
   "source": [
    "### Call TongYi to judge the dup - single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3394757d-c9d0-463b-86ab-118cad0fdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http import HTTPStatus\n",
    "import dashscope\n",
    "import os\n",
    "\n",
    "def call_tongyi(query_content, stream_mode = False):\n",
    "    dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'user', 'content': query_content}]\n",
    "\n",
    "\n",
    "    rsp_str = ''\n",
    "    \n",
    "    if stream_mode:\n",
    "        # streaming mode. If processing time is long, this can be used to get partial response when still processing\n",
    "        responses = dashscope.Generation.call(\"qwen-max\",\n",
    "                                    messages=messages,\n",
    "                                    result_format='message',  # set the result to be \"message\"  format.\n",
    "                                    stream=True, # set streaming output\n",
    "                                    incremental_output=True  # get streaming output incrementally\n",
    "                                    )\n",
    "        for response in responses:\n",
    "            if response.status_code == HTTPStatus.OK:\n",
    "                #print(response.output.choices[0]['message']['content'],end='')\n",
    "                rsp_str = rsp_str + response.output.choices[0]['message']['content']\n",
    "            else:\n",
    "                print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "                    response.request_id, response.status_code,\n",
    "                    response.code, response.message\n",
    "                ))\n",
    "    else:\n",
    "        # no need to use streaming mode. See whether this can save processing time\n",
    "        response = dashscope.Generation.call(\"qwen-max\",\n",
    "                                    messages=messages,\n",
    "                                    result_format='message'\n",
    "                                    )\n",
    "        if response.status_code == HTTPStatus.OK:\n",
    "            #print(response.output.choices[0]['message']['content'],end='')\n",
    "            rsp_str = response.output.choices[0]['message']['content']\n",
    "        else:\n",
    "            print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "                response.request_id, response.status_code,\n",
    "                response.code, response.message\n",
    "            ))\n",
    "    \n",
    "\n",
    "    rsp_json_data = parse_rsp(rsp_str)\n",
    "\n",
    "    #print(rsp_str, '\\n\\n')\n",
    "    #print('----->>> Compared with parsed data ----->>> \\n')\n",
    "    #print(f'{rsp_json_data}')\n",
    "\n",
    "    return rsp_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6a11274-00fd-435c-8ec4-0c7bfac86f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: rsp_json_data = {'Probability': '95%', 'Analysis': 'Both tickets essentially request the same core functionality, which is the ability to import issues from a CSV file into multiple projects simultaneously, with the project identifier being specified in a dedicated column within the CSV. The descriptions vary slightly but convey the same user need for improved efficiency when importing issues across different projects.', 'New Requirement': \"* Subject: Enhance CSV Import to Support Multiple Projects\\n    * Description: Enhance the existing issue import feature to facilitate the import of issues from a single CSV file into multiple projects. This enhancement will include the option to map a designated column in the CSV to the 'Project' field, enabling users to specify the target project for each issue directly within the import file. This improvement aims to streamline the import process and allow users to efficiently manage issue creation across different projects in a single action.\"}\n",
      "Test 2: rsp_json_data = {'Probability': '5%', 'Analysis': 'The two tickets are addressing completely different features within Redmine. The first ticket is about implementing keyboard shortcuts to enhance user productivity, while the second ticket is focused on introducing auto-watch functionality for issues based on user actions and assignments. Since they target distinct aspects of the application, they should not be marked as duplicates.', 'New Requirement': 'Not applicable since the probability of duplication is below 70%. However, if we were to imagine a scenario where these could somehow converge, which is not recommended, it might look like:\\n\\n** Subject: Enhanced User Productivity Features\\n\\n** Description: Implement enhancements to improve user workflow within Redmine, including:\\n- Keyboard shortcuts for common actions such as previewing (Ctrl+R), submitting changes (Ctrl+S), and initiating a search (Ctrl+/). This will involve adding \"accesskey\" attributes to relevant UI elements.\\n- A customizable auto-watch system allowing users to set personal preferences for automatically watching issues they\\'ve modified or those assigned to them, inspired by Patch #222, with additional options for users to define their own auto-watch policies.\\n\\nRemember, this combined requirement is not advised due to the distinct nature of the original requests.'}\n",
      "Test 3: rsp_json_data = {'Probability': '95%', 'Analysis': 'Both requirements essentially aim to enable the assignment of tasks or issues to multiple users. The descriptions provided highlight similar use cases and desires for improving workflow efficiency by allowing multiple assignees. The only minor difference is the additional detail in Ticket1 about a \"being solved\" state, which could be seen as an extension of the core functionality requested in both tickets.', 'New Requirement': '* Subject: Implement Multi-Assignment for Tasks and Issues\\n    * Description: Enhance the platform to allow tasks and issues to be assigned to multiple users simultaneously. This will include an update to the \"Assignee\" field, transforming it into a list box with multi-selection capability, enabling users to choose multiple assignees from any combination of team members, regardless of their grouping. Additionally, introduce a new \"Being Solved\" status to indicate when a task has been picked up by one of the assigned users, locking the task and signaling that it is actively being addressed. This improvement will cater to scenarios where multiple workers can resolve a task and promote efficient task distribution among available team members.'}\n"
     ]
    }
   ],
   "source": [
    "### test ... function()\n",
    "\n",
    "# The source requirements file\n",
    "df = pd.read_csv('all_issues_for_test.csv')\n",
    "\n",
    "# Test 1\n",
    "req1 = df.iloc[435].to_dict()\n",
    "req2 = df.iloc[170].to_dict()\n",
    "query_content = form_query(req1, req2)\n",
    "rsp_json_data = call_tongyi(query_content, stream_mode = False)\n",
    "print(f'Test 1: rsp_json_data = {rsp_json_data}')\n",
    "\n",
    "# Test 2\n",
    "req1 = df.iloc[1].to_dict()\n",
    "req2 = df.iloc[10].to_dict()\n",
    "query_content = form_query(req1, req2)\n",
    "rsp_json_data = call_tongyi(query_content, stream_mode = False)\n",
    "print(f'Test 2: rsp_json_data = {rsp_json_data}')\n",
    "\n",
    "# Test 2\n",
    "req1 = df.iloc[0].to_dict()\n",
    "req2 = df.iloc[66].to_dict()\n",
    "query_content = form_query(req1, req2)\n",
    "rsp_json_data = call_tongyi(query_content, stream_mode = False)\n",
    "print(f'Test 3: rsp_json_data = {rsp_json_data}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac69b0-4441-4d5a-bb1d-f625deb9f0f0",
   "metadata": {},
   "source": [
    "### Generate batch request files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af5cf5be-dada-4478-b8d7-d146097e6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_request_files(src_req_csv_file_name, cor_json_file_name):\n",
    "    # How many requests in a batch\n",
    "    BATCH_SIZE = 500\n",
    "    INPUT_FILE_PREFIX = 'test/input_files_1/req_batch_input'\n",
    "    \n",
    "    # The source requirements file\n",
    "    df = pd.read_csv(src_req_csv_file_name)\n",
    "    #print(df.head())\n",
    "    \n",
    "    # The source corr file\n",
    "    cor_data = load_array_from_json(cor_json_file_name)\n",
    "    NUM_OF_REQ = len(cor_data)\n",
    "    #print(f'=== NUM_OF_REQ = {NUM_OF_REQ} ===\\n')\n",
    "    #print(f'cor_data[0] = {cor_data[0]}\\n')\n",
    "    \n",
    "    # initialize\n",
    "    count = 0\n",
    "    file_index = 0\n",
    "    rows = []\n",
    "    \n",
    "    # prepare the data for every pair or requirements, with correlation coefficient > 0.5\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        for j in range(i):\n",
    "            #print(f'Doing i = {i}, j = {j}\\n')\n",
    "            \n",
    "            if cor_data[i][j] > 0.5:\n",
    "                count += 1\n",
    "                \n",
    "                custom_id = str(count)\n",
    "                req1 = df.iloc[i].to_dict()\n",
    "                req2 = df.iloc[j].to_dict()\n",
    "                query_content = form_query(req1, req2)\n",
    "                row = {\t\"custom_id\": custom_id, \n",
    "                    \t\"method\": \"POST\", \n",
    "                    \t\"url\": \"/v1/chat/completions\", \n",
    "                    \t\"body\": {\t\"model\": \"qwen-max\", \n",
    "                    \t\t\t\t\"messages\": [{\"role\": \"user\", \"content\": query_content}]\n",
    "                    \t\t\t}\n",
    "                      }\n",
    "                #print(f'row = {row}')\n",
    "                #print(f'row = {rows}')\n",
    "                rows.append(row)\n",
    "\n",
    "\n",
    "            if (count == BATCH_SIZE):\n",
    "                file_index += 1\n",
    "                file_name = INPUT_FILE_PREFIX + str(file_index) + '.json'\n",
    "                save_json_in_lines(rows, file_name)\n",
    "                print(f'Finished writing to file: {file_name}')\n",
    "\n",
    "                count = 0\n",
    "                rows = []\n",
    "\n",
    "                # for debugging\n",
    "                #if file_index == 3:\n",
    "                #    return\n",
    "\n",
    "    if (count > 0) and (count <= BATCH_SIZE):\n",
    "        file_index += 1\n",
    "        file_name = INPUT_FILE_PREFIX + str(file_index) + '.json'\n",
    "        save_json_in_lines(rows, file_name)\n",
    "        print(f'Finished writing to file: {file_name}')\n",
    "\n",
    "        count = 0\n",
    "        rows = []\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99399114-7129-431d-a65c-e23f60e936cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to file: test/input_files_1/req_batch_input1.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input2.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input3.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input4.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input5.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input6.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input7.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input8.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input9.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input10.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input11.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input12.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input13.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input14.json\n",
      "Finished writing to file: test/input_files_1/req_batch_input15.json\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "src_req_csv_file_name = 'all_issues_for_test.csv'\n",
    "cor_json_file_name = 'req_cor.json'\n",
    "\n",
    "generate_batch_request_files(src_req_csv_file_name, cor_json_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d93f0-9a01-43c2-9f59-4b69f04727a5",
   "metadata": {},
   "source": [
    "### async mode: can send multiple prompts and wait for response in async mode\n",
    "But as we test, this does not help to reduce the response time when sending prompts to TongYi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a028d26-b79e-4449-afe3-1bcbe52e3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import platform\n",
    "from dashscope.aigc.generation import AioGeneration\n",
    "\n",
    "import dashscope\n",
    "dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# for debugging usage\n",
    "async def task(question):\n",
    "    #print(f\"Sending question: {question}\")\n",
    "    #response = await AioGeneration.call(\"qwen-turbo\", prompt=question)\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': question}]\n",
    "    response = dashscope.Generation.call(\"qwen-max\",\n",
    "                                messages=messages,\n",
    "                                result_format='message'\n",
    "                                )\n",
    "    #print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "# 主异步函数\n",
    "async def main(questions):\n",
    "    #questions = [\"你是谁？\", \"你会什么？\", \"天气怎么样？\"]\n",
    "    #questions = [str1, str2, str3]\n",
    "    tasks = [task(q) for q in questions]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e664fd7-2e24-42d3-8343-1f9edddb239d",
   "metadata": {},
   "source": [
    "### Query and Save -- async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7175054c-78c2-48da-bf3d-1be888f5b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug with a few records\n",
    "DEBUG_STOP_AT = 10\n",
    "def query_and_save(src_req_csv_file_name, cor_json_file_name, result_csv_file_name):\n",
    "    # How many requests in a batch\n",
    "    BATCH_SIZE = 5\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # The source requirements file\n",
    "    df = pd.read_csv(src_req_csv_file_name)\n",
    "    \n",
    "    # 1. read cor from cor_json_file_name\n",
    "    cor_data = load_array_from_json(cor_json_file_name)\n",
    "    NUM_OF_REQ = len(cor_data)\n",
    "    print(f'-------- array size: {NUM_OF_REQ} x {NUM_OF_REQ} --------\\n')\n",
    "\n",
    "\n",
    "    # 打开 CSV 文件进行写入\n",
    "    header = ['Index1', 'Issue1 ID', 'Issue1 Subject', 'Issue1 Description',\n",
    "            'Index2', 'Issue2 ID', 'Issue2 Subject', 'Issue2 Description',\n",
    "            'Embedding Corr', 'LLM Probability', 'Analysis', 'New Requirement']\n",
    "\n",
    "    questions = []\n",
    "    infos = []\n",
    "    with open(result_csv_file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # 写入表头\n",
    "        writer.writerow(header)\n",
    "        print(f'\\nHeader is saved:\\n{header}')\n",
    "    \n",
    "        # 遍历计算每两个需求之间的相似度（点积）\n",
    "        check_count = 0\n",
    "        record_count = 0\n",
    "        num_in_batch = 0\n",
    "        \n",
    "        for i in range(NUM_OF_REQ):\n",
    "            for j in range(i):\n",
    "                #print(f'Doing i = {i}, j = {j}\\n')\n",
    "                check_count += 1\n",
    "                if (check_count % 500 == 0):\n",
    "                    # for debug\n",
    "                    #if check_count == 1000:\n",
    "                    #    return\n",
    "                    \n",
    "                    # 记录结束时间\n",
    "                    end_time = time.time()\n",
    "                    # 计算并输出耗时\n",
    "                    elapsed_time = end_time - start_time\n",
    "                    print(f'Now we have checked {check_count} of pairs, in {elapsed_time:.4f} seconds\\n')\n",
    "                \n",
    "                if(cor_data[i][j] > 0.5):\n",
    "                    print(f'handling cor_data[{i}][{j}] = {cor_data[i][j]}\\n')\n",
    "                    req1 = df.iloc[i].to_dict()\n",
    "                    req2 = df.iloc[j].to_dict()\n",
    "                    query_content = form_query(req1, req2)\n",
    "                    req_info = {\n",
    "                        'Index1':i, 'Issue1 ID':req1['id'], \n",
    "                        'Issue1 Subject':req1['subject'], 'Issue1 Description':req1['description'],\n",
    "                        'Index2':j, 'Issue2 ID':req2['id'], \n",
    "                        'Issue2 Subject':req2['subject'], 'Issue2 Description':req2['description'],\n",
    "                        'Embedding Corr':cor_data[i][j]\n",
    "                    }\n",
    "                    #print(f'query_content = {query_content}\\n')\n",
    "                    #print(f'req_info = {req_info}\\n')\n",
    "                    \n",
    "                    '''\n",
    "                    # this is single call\n",
    "                    rsp_json_data = call_tongyi_with_stream(query_content)\n",
    "                    row = [i, req1['id'],req1['subject'],req1['description'],\n",
    "                           j, req2['id'],req2['subject'],req2['description'],\n",
    "                           cor_data[i][j],rsp_json_data['Probability'],\n",
    "                           rsp_json_data['Analysis'],rsp_json_data['New Requirement']\n",
    "                          ]\n",
    "                    writer.writerow(row)\n",
    "                    record_count += 1\n",
    "                    '''\n",
    "\n",
    "                    # this is batch async call\n",
    "                    questions.append(query_content)\n",
    "                    infos.append(req_info)\n",
    "                    num_in_batch += 1\n",
    "                    #print(f'num_in_batch = {num_in_batch}\\n')\n",
    "\n",
    "                    # num_in_batch controls\n",
    "                    if num_in_batch == BATCH_SIZE:\n",
    "                        if __name__ == '__main__':\n",
    "                            #print(f'Will do async call now for i = {i}, j = {j}')\n",
    "                            #asyncio.get_event_loop().run_until_complete(main(questions, infos, writer))\n",
    "                            results = asyncio.get_event_loop().run_until_complete(main(questions))\n",
    "\n",
    "                            result_idx = 0\n",
    "                            for response in results:\n",
    "                                #print(f'\\nAsync call is returned, response = \\n{response}\\n\\n')\n",
    "                                if response.status_code == HTTPStatus.OK:\n",
    "                                    #print(response.output.choices[0]['message']['content'],end='')\n",
    "                                    rsp_str = response.output.choices[0]['message']['content']\n",
    "\n",
    "                                    rsp_json_data = parse_rsp(rsp_str)\n",
    "                                    info = infos[result_idx]\n",
    "                                    row = [info['Index1'], info['Issue1 ID'], info['Issue1 Subject'], info['Issue1 Description'],\n",
    "                                           info['Index2'], info['Issue2 ID'], info['Issue2 Subject'], info['Issue2 Description'],\n",
    "                                           info['Embedding Corr'],\n",
    "                                           rsp_json_data['Probability'], rsp_json_data['Analysis'],rsp_json_data['New Requirement']\n",
    "                                          ]\n",
    "                                    writer.writerow(row)\n",
    "                                else:\n",
    "                                    print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "                                        response.request_id, response.status_code,\n",
    "                                        response.code, response.message\n",
    "                                    ))\n",
    "                                    \n",
    "                                result_idx += 1\n",
    "                                \n",
    "                            #print(f'async call is done: {result_idx+1} lines are saved')\n",
    "                        \n",
    "                        else:  # if __name__ == '__main__':\n",
    "                            print(f'not called in __main__, do nothing')\n",
    "\n",
    "                        record_count += BATCH_SIZE\n",
    "                        num_in_batch = 0\n",
    "                        questions = []\n",
    "                        infos = []\n",
    "                        \n",
    "                    else:  # if num_in_batch == BATCH_SIZE:\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "                    if (record_count % 10 == 0):\n",
    "                        # 记录结束时间\n",
    "                        end_time = time.time()\n",
    "                        # 计算并输出耗时\n",
    "                        elapsed_time = end_time - start_time\n",
    "                        print(f'Now we have saved {record_count} results, in {elapsed_time:.4f} seconds\\n')\n",
    "\n",
    "                    if record_count >= DEBUG_STOP_AT:\n",
    "                        return\n",
    "                    \n",
    "\n",
    "                    # for test purpose\n",
    "                    #if (record_count >= 30):\n",
    "                    #    return\n",
    "\n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    # 计算并输出耗时\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Totally {record_count} results are saved to {result_csv_file_name}, in {elapsed_time:.4f} seconds')\n",
    "    return\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5fc4c46-5d16-403b-9df9-dafcc715ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- array size: 743 x 743 --------\n",
      "\n",
      "\n",
      "Header is saved:\n",
      "['Index1', 'Issue1 ID', 'Issue1 Subject', 'Issue1 Description', 'Index2', 'Issue2 ID', 'Issue2 Subject', 'Issue2 Description', 'Embedding Corr', 'LLM Probability', 'Analysis', 'New Requirement']\n",
      "handling cor_data[9][3] = 0.5248227168451436\n",
      "\n",
      "handling cor_data[14][4] = 0.5009113902169107\n",
      "\n",
      "handling cor_data[14][9] = 0.5587199102244673\n",
      "\n",
      "handling cor_data[18][14] = 0.5172098519420762\n",
      "\n",
      "handling cor_data[19][5] = 0.5160936046872969\n",
      "\n",
      "handling cor_data[23][10] = 0.5368007963526455\n",
      "\n",
      "handling cor_data[23][12] = 0.7090704143593299\n",
      "\n",
      "handling cor_data[24][9] = 0.5124424719444702\n",
      "\n",
      "handling cor_data[25][6] = 0.6336000244467357\n",
      "\n",
      "handling cor_data[26][18] = 0.541367402405958\n",
      "\n",
      "Now we have saved 10 results, in 114.6025 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test async mode, and output to file \n",
    "query_and_save('all_issues_for_test.csv', 'req_cor.json', 'temp_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda2044-6dfe-44f2-9b64-ea6f1b4b8f71",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "854996d6-16ae-4536-ad8b-a706c49b3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== NeedRecalc: False ===\n",
      "\n",
      "\n",
      "type(req_embeddings) = <class 'list'>\n",
      "type(req_cor) = <class 'list'>\n",
      "type(l2_norm) = <class 'list'>\n",
      "type(consine_similarity) = <class 'list'>\n",
      "\n",
      "\n",
      " ~~~~~ Totally 7154 for correlation coefficient > 0.5 ~~~~~ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ~~~~~ Totally 7154 for consine similarity > 0.5 ~~~~~ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Files for Query TongYi\n",
    "src_req_csv_file_name = 'all_issues_for_test.csv'\n",
    "embedding_json_file_name = 'req_embeddings.json'\n",
    "cor_json_file_name = 'req_cor.json'\n",
    "l2_norm_json_file_name = 'req_l2_norm.json'\n",
    "cosine_similarity_json_file_name = 'req_cosine_similarity.json'\n",
    "result_csv_file_name = 'result.csv'\n",
    "\n",
    "# macro to control whether need recalculate the embeddings and the correlation coefficient\n",
    "NeedRecalc = False\n",
    "VERBASE_MODE = False\n",
    "\n",
    "\n",
    "# if embedding or cor file doesn't exist, force recalc\n",
    "if not os.path.exists(embedding_json_file_name):\n",
    "    NeedRecalc = True\n",
    "if not os.path.exists(cor_json_file_name):\n",
    "    NeedRecalc = True\n",
    "\n",
    "print(f'\\n\\n=== NeedRecalc: {NeedRecalc} ===\\n\\n')\n",
    "\n",
    "if NeedRecalc:\n",
    "    # input csv file which contains all requirements\n",
    "    csv_file_name = 'all_issues_for_test.csv'\n",
    "    \n",
    "    # Prepare the embedding\n",
    "    embedding_model = prepare_embedding_model()\n",
    "    \n",
    "    # Calc the embeddings\n",
    "    req_embeddings = calc_embedding_from_csv(csv_file_name, embedding_model)\n",
    "    \n",
    "    # Calc the correlation coefficient\n",
    "    req_cor = calc_corr(req_embeddings)\n",
    "\n",
    "    # save embedding and cor\n",
    "    save_array_to_json(req_embeddings, embedding_json_file_name)\n",
    "    save_array_to_json(req_cor, cor_json_file_name)\n",
    "    \n",
    "    # calc l2_norm and consine_similarity\n",
    "    l2_norm = calc_l2_norm(req_embeddings)\n",
    "    consine_similarity = calc_consine_similarity(req_cor, l2_norm)\n",
    "    # save l2_norm and consine_similarity\n",
    "    save_array_to_json(l2_norm, l2_norm_json_file_name)\n",
    "    save_array_to_json(consine_similarity, cosine_similarity_json_file_name)\n",
    "else:\n",
    "    req_embeddings = load_array_from_json(embedding_json_file_name)\n",
    "    req_cor = load_array_from_json(cor_json_file_name)\n",
    "    print(f'type(req_embeddings) = {type(req_embeddings)}')\n",
    "    print(f'type(req_cor) = {type(req_cor)}')\n",
    "\n",
    "    l2_norm = load_array_from_json(l2_norm_json_file_name)\n",
    "    consine_similarity = load_array_from_json(cosine_similarity_json_file_name)\n",
    "    print(f'type(l2_norm) = {type(l2_norm)}')\n",
    "    print(f'type(consine_similarity) = {type(consine_similarity)}')\n",
    "\n",
    "NUM_OF_REQ = len(req_embeddings)\n",
    "\n",
    "# 打印超过0.5的相关系数\n",
    "total_cor_num = 0\n",
    "for i in range(NUM_OF_REQ):\n",
    "    print_row_num = False\n",
    "    for j in range(i):\n",
    "        if req_cor[i][j] > 0.5:\n",
    "            total_cor_num += 1\n",
    "            if (VERBASE_MODE):    \n",
    "                if not print_row_num:\n",
    "                    print_row_num = True\n",
    "                    print(f'\\nRow: NO.{i}:')\n",
    "                print(f'[{i}][{j}] = {req_cor[i][j]}')\n",
    "                \n",
    "print(f\"\\n\\n ~~~~~ Totally {total_cor_num} for correlation coefficient > 0.5 ~~~~~ \\n\\n\")\n",
    "\n",
    "\n",
    "# 打印超过0.5的consine similarity\n",
    "total_consine_similarity_num = 0\n",
    "for i in range(NUM_OF_REQ):\n",
    "    print_row_num = False\n",
    "    for j in range(i):\n",
    "        if consine_similarity[i][j] > 0.5:\n",
    "            total_consine_similarity_num += 1\n",
    "            if (VERBASE_MODE):    \n",
    "                if not print_row_num:\n",
    "                    print_row_num = True\n",
    "                    print(f'\\nRow: NO.{i}:')\n",
    "                print(f'[{i}][{j}] = {req_cor[i][j]}')\n",
    "    \n",
    "print(f\"\\n\\n ~~~~~ Totally {total_consine_similarity_num} for consine similarity > 0.5 ~~~~~ \\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b04e90-47e9-473e-8ea6-1a54b803ccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ee73c-f8d3-4502-a690-45658981807d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
