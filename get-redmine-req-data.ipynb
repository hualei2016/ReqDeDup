{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4402d5-cc34-4394-b56d-f44ea4dc4581",
   "metadata": {},
   "source": [
    "# Step 1: Utility functions to prepare the requirement data file: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fce052-b5b2-4f85-a7c0-1f898c55bec5",
   "metadata": {},
   "source": [
    "# Get redmine requirement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72ed2a0-0341-4b0d-b10c-b5d45338a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def query_and_save_duplicated_issues(src_file, dst_file, verbose = False):\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(src_file)\n",
    "    \n",
    "    # 打印前几条需求数据\n",
    "    if verbose:\n",
    "        print(df.head())\n",
    "    \n",
    "    # init the json_data (a list of dict)\n",
    "    json_data = []\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for index, row in df.iterrows():\n",
    "        if (index + 1) % 20 == 0:\n",
    "            print(f'{index+1} records are processed.')\n",
    "            \n",
    "        # debug info flag\n",
    "        if ((index + 1) % 20 == 0) and verbose:\n",
    "            debug_info_flag = True\n",
    "        else:\n",
    "            debug_info_flag = False\n",
    "        \n",
    "        req = row.to_dict()\n",
    "        related_issues = req['Related issues']\n",
    "    \n",
    "        # 使用split方法按逗号分隔字符串\n",
    "        if (isinstance(related_issues, str)):\n",
    "            related_issue_strings = related_issues.split(',')\n",
    "        else:\n",
    "            if debug_info_flag:\n",
    "                print(f\"Requirement {index + 1}: related issues info is NOT a string\")\n",
    "            related_issue_strings = []    # if 'Related issues' is empty, the value may be float type Nan\n",
    "\n",
    "        if debug_info_flag:\n",
    "            print(f\"\\n\\nProcessing Requirement NO.{index + 1}: \\n\")\n",
    "            print(f\"Related issues: {related_issues}\")\n",
    "            print(f\"After split, related_issue_strings = {related_issue_strings}\")\n",
    "    \n",
    "    \n",
    "        for issue_string in related_issue_strings:\n",
    "            # 使用正则表达式提取数字\n",
    "            duplicate_number = -1\n",
    "            if (isinstance(issue_string, str)):\n",
    "                match = re.search(r'Is duplicate of #(\\d+)', issue_string)\n",
    "                if (not match):\n",
    "                    match = re.search(r'Has duplicate #(\\d+)', issue_string)\n",
    "                if (not match):\n",
    "                    match = re.search(r'Copied from #(\\d+)', issue_string)\n",
    "            \n",
    "                # 检查是否匹配成功\n",
    "                if match:\n",
    "                    duplicate_number = match.group(1)\n",
    "                    #print(f\"The duplicate ticket number is: {duplicate_number}\")\n",
    "                else:\n",
    "                    if debug_info_flag:\n",
    "                        print(f\"Requirement {index + 1}: substring {issue_string}: is NOT duplicate relation\")\n",
    "            else:\n",
    "                if debug_info_flag:\n",
    "                    print(f\"Requirement {index + 1}: substring {issue_string}: is NOT a string\")\n",
    "        \n",
    "            # This row has no duplicated number\n",
    "            if (-1 == duplicate_number):\n",
    "                continue\n",
    "        \n",
    "            # 发送 GET 请求, 获取related issue\n",
    "            req = 'https://www.redmine.org/issues/'\n",
    "            req = req + duplicate_number + '.json?include=relations'\n",
    "        \n",
    "            response = requests.get(req)\n",
    "            #print(\"response.status_code: \", response.status_code)\n",
    "        \n",
    "            # 检查请求是否成功\n",
    "            if response.status_code == 200:\n",
    "                # 处理响应内容\n",
    "                data_out = response.json()  # 如果响应是 JSON 格式\n",
    "                data_issue = data_out['issue']\n",
    "                if debug_info_flag:\n",
    "                    formatted_json = json.dumps(data_out, indent=4)\n",
    "                    print(f'data_out = {formatted_json}')\n",
    "        \n",
    "                relation_json = data_issue['relations']\n",
    "                # 构建duplicate信息\n",
    "                dup_str = ''\n",
    "                count = 0\n",
    "                for rel in relation_json:\n",
    "                    # 只提取dup的relation信息，忽略其他：如related\n",
    "                    #print('\\n###relation_type = ', rel['relation_type'])\n",
    "                    if rel['relation_type'] == 'duplicates':\n",
    "                        #print(rel)\n",
    "    \n",
    "                        # redmine differentiates 'duplicates' and 'duplicate to'\n",
    "                        if rel['issue_id'] == duplicate_number:\n",
    "                            dup_id = rel['issue_to_id']\n",
    "                        else:\n",
    "                            dup_id = rel['issue_id']\n",
    "                            \n",
    "                        if debug_info_flag:\n",
    "                            print(f'\\n*** dup_id = {dup_id}')\n",
    "                        \n",
    "                        if count > 0:\n",
    "                            dup_str = dup_str + ', '\n",
    "                        dup_str = dup_str + 'Is duplicate of #' + str(dup_id)\n",
    "                        #print(f\"\\n--- dup_str is {dup_str} ---\\n\\n\")\n",
    "        \n",
    "                        count = count + 1\n",
    "        \n",
    "        \n",
    "                \n",
    "                # 重构一个json，只包含我们需要的数据\n",
    "                data = {\n",
    "                    'id':data_issue['id'],\n",
    "                    'subject':data_issue['subject'],\n",
    "                    'author':data_issue['author']['name'],\n",
    "                    'status':data_issue['status']['name'],\n",
    "                    'related issues':dup_str,\n",
    "                    'description':data_issue['description']\n",
    "                }\n",
    "                \n",
    "                #print(data)\n",
    "                #print(type(data))\n",
    "                json_data.append(data)\n",
    "            else:\n",
    "                print(f\"Error: For Requirement {index + 1}, when trying to get dup issue, get {response.status_code}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write data of the duplicated issue into the file\n",
    "    fieldnames = [\"id\", \"subject\", \"author\", \"status\", \"related issues\", \"description\"]\n",
    "    with open(dst_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        \n",
    "        # 创建 CSV 写入器\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # 写入 CSV 文件的表头\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 遍历每条 JSON 数据\n",
    "        for entry in json_data:\n",
    "            # 提取所需字段，如果字段不存在则使用默认值 None 或空字符串\n",
    "            row = {field: entry.get(field, None) for field in fieldnames}\n",
    "            # 写入到 CSV 文件\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ae2ee90-0fce-4626-8ec7-02b6f9aa1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 records are processed.\n",
      "40 records are processed.\n",
      "60 records are processed.\n",
      "80 records are processed.\n",
      "100 records are processed.\n",
      "120 records are processed.\n",
      "140 records are processed.\n",
      "160 records are processed.\n",
      "180 records are processed.\n",
      "200 records are processed.\n"
     ]
    }
   ],
   "source": [
    "src_file = 'issues-closed-duplicated.csv'\n",
    "dst_file = 'dup_issues_downloaded.csv'\n",
    "\n",
    "query_and_save_duplicated_issues(src_file, dst_file, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ad092-ca2b-4a06-8859-0ef1b766c595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb21f2-8a8a-4177-91e8-318b534b12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort and dedup a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73696d3-f236-441e-bf63-fa74306dc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_dedup_csv(input_csv_name, output_csv_name, sort_col_name, dedup_col_name):\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(input_csv_name)\n",
    "    \n",
    "    # 按指定字段排序，例如按 'column_name' 字段\n",
    "    df_sorted = df.sort_values(by=sort_col_name)\n",
    "    \n",
    "    # 根据某个字段去重，例如按 'column_name' 字段去重\n",
    "    df_unique = df_sorted.drop_duplicates(subset=sort_col_name)\n",
    "    \n",
    "    # 将结果保存到新的 CSV 文件中\n",
    "    df_unique.to_csv(output_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241ab98-89f3-4b5c-905d-d71d66f7552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### used to sort and remove duplicated records. shouldn't do harm if run multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c852363-2b8e-4818-b143-b9a6863deec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_and_dedup_csv('all_issues_for_test-backup.csv', 'all_issues_for_test,csv', 'id', 'id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
