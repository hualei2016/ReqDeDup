{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb9e41-2a64-4718-8c4b-d66f0ff1a6a4",
   "metadata": {},
   "source": [
    "# Step 4: Result analysis - Calculate precission, recall and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a0cd8-e3d8-404f-be1f-b99998a3c9bb",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b712b187-80a6-4876-bd02-d5a9a9aaa67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd4efb-cce1-48e0-97fc-2b0acaffc446",
   "metadata": {},
   "source": [
    "## 1. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ea315-8c73-40fd-bcc4-c7b27fbd457a",
   "metadata": {},
   "source": [
    "### 1.1 functions for loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3a11ecb-64ae-45cd-b5eb-d5bd05067b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 打开文件并逐行读取每个 JSON 对象\n",
    "def load_json_in_lines(file_name):\n",
    "    json_array = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())  # 解析 JSON 并添加到数组中\n",
    "            #print(json_obj)\n",
    "            json_array.append(json_obj)\n",
    "    \n",
    "    #print(\"从文件读取的JSON数据：\")\n",
    "\n",
    "    return json_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1515349a-3e69-4bd5-804f-19bec0be8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_array_from_json(file_name):\n",
    "    # 从 JSON 文件中读取一维数组\n",
    "    with open(file_name, mode='r') as file:\n",
    "        array = json.load(file)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd952efa-34da-4475-ad1e-a908640d9e37",
   "metadata": {},
   "source": [
    "### 1.2 save json data line by line into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fec384f-4fcf-4f90-90de-c5b5a7ebfe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# 打开文件并逐行写入 JSON 数据\n",
    "def save_jsons_array_into_csv(json_array, csv_file_path):\n",
    "\n",
    "    # 提取 JSON 对象的字段名称作为 CSV 文件的表头\n",
    "    fieldnames = json_array[0].keys()\n",
    "    \n",
    "    # 写入 CSV 文件\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # 写入表头\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 写入 JSON 数组中的每个对象\n",
    "        for json_obj in json_array:\n",
    "            writer.writerow(json_obj)\n",
    "    \n",
    "    print(f\"JSON data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3f4a7ea-f722-47a9-b397-6546b9379470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been written to output_json_in_csv.csv\n"
     ]
    }
   ],
   "source": [
    "# test save_jsons_array_into_csv\n",
    "\n",
    "json_array = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Los Angeles\"}\n",
    "]\n",
    "\n",
    "# 指定要写入的 CSV 文件路径\n",
    "csv_file_path = 'output_json_in_csv.csv'\n",
    "\n",
    "save_jsons_array_into_csv(json_array, csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4cfbf-03f9-47f6-b34b-5f00552fea6f",
   "metadata": {},
   "source": [
    "### 1.3 function to get duplicated IDs from the string in relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "deb2cfe7-a800-4988-ba9a-cd0ae3f11f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test regexp\n",
    "import re\n",
    "\n",
    "# 使用正则表达式匹配特定模式后的数字\n",
    "def get_dup_ids(text, verbose = False):\n",
    "    matches = []\n",
    "    if isinstance(text, str):    # 输入有可能不是字符串，可能是float类型的nan\n",
    "        matches = re.findall(r'(?:Is duplicate of #|Copied from #|Has duplicate #)(\\d+)', text)\n",
    "\n",
    "    # 输出匹配结果\n",
    "    if verbose:\n",
    "        print(f'In {inspect.currentframe().f_code.co_name}: text = {text}')\n",
    "        print(f'matches = {matches}')\n",
    "\n",
    "    return matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68db8e1c-396f-48ee-bc2a-64a78ac49a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get_dup_ids: text = Is duplicate of #12002, Is duplicate of #35509\n",
      "matches = ['12002', '35509']\n",
      "In get_dup_ids: text = Is duplicate of #10230, Related to #18857, Copied from #33325\n",
      "matches = ['10230', '33325']\n",
      "In get_dup_ids: text = Blocked by #38273, Has duplicate #15148, Follows #37987, Is duplicate of #10091\n",
      "matches = ['15148', '10091']\n",
      "In get_dup_ids: text = Related to #18857\n",
      "matches = []\n",
      "In get_dup_ids: text = Has duplicate #15148\n",
      "matches = ['15148']\n",
      "In get_dup_ids: text = Copied from #33325\n",
      "matches = ['33325']\n",
      "In get_dup_ids: text = Blocked by #38273\n",
      "matches = []\n",
      "In get_dup_ids: text = Follows #37987\n",
      "matches = []\n",
      "In get_dup_ids: text = Is duplicate of #10091\n",
      "matches = ['10091']\n"
     ]
    }
   ],
   "source": [
    "# test function get_dup_ids()\n",
    "texts = [\n",
    "'''Is duplicate of #12002, Is duplicate of #35509''',\n",
    "'''Is duplicate of #10230, Related to #18857, Copied from #33325''',\n",
    "'''Blocked by #38273, Has duplicate #15148, Follows #37987, Is duplicate of #10091''',\n",
    "'''Related to #18857''',\n",
    "'''Has duplicate #15148''',\n",
    "'''Copied from #33325''',\n",
    "'''Blocked by #38273''',\n",
    "'''Follows #37987''',\n",
    "'''Is duplicate of #10091''',\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    get_dup_ids(text, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a2685-4fbd-46c5-bc1c-709dc4a907e0",
   "metadata": {},
   "source": [
    "## 2. Retrive base and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bc775-724b-4da5-baf2-76166318d94f",
   "metadata": {},
   "source": [
    "### 2.1 utility: given the issue id, try to find index in the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff81253-c7d2-4afa-bb75-d8a255cbcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the record, by issue_id\n",
    "def find_index_by_issue_id(src_req_csv_file_name, issue_id, verbose = False):\n",
    "    # The source requirements file\n",
    "    df = pd.read_csv(src_req_csv_file_name)\n",
    "    NUM_OF_REQ = df.shape[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f'NUM_OF_REQ = {NUM_OF_REQ}')\n",
    "        print(f'issue_id: type = {type(issue_id)}, value = {issue_id}')\n",
    "\n",
    "    for i  in range(NUM_OF_REQ):\n",
    "        req = df.iloc[i].to_dict()\n",
    "        if int(issue_id) == int(req['id']):\n",
    "            return i\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d8cb2-c30f-4297-97c7-064eeb357796",
   "metadata": {},
   "source": [
    "### 2.2 retrieve the base array, which stores the duplication of the example datas\n",
    "input: src_req_csv_file_name is a csv file, each line contains a requirement\n",
    "\n",
    "output: base[i][j] = 1 means req[i] and req[j] are duplicated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "096767ab-7689-4271-b0ce-68db144412b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_base(src_req_csv_file_name, verbose = False):\n",
    "    # How many requests in a batch\n",
    "    BATCH_SIZE = 500\n",
    "    \n",
    "    # The source requirements file\n",
    "    df = pd.read_csv(src_req_csv_file_name)\n",
    "    NUM_OF_REQ = df.shape[0]\n",
    "    print(f'=== NUM_OF_REQ = {NUM_OF_REQ} ===\\n')\n",
    "\n",
    "    #print(df.head())\n",
    "\n",
    "    base = np.zeros((NUM_OF_REQ, NUM_OF_REQ))\n",
    "    for i  in range(NUM_OF_REQ):\n",
    "        req = df.iloc[i].to_dict()\n",
    "        issue_id = req['id']    # string type\n",
    "        related_issues = req['related issues']    # string type\n",
    "\n",
    "        if verbose:\n",
    "            print(f'issue_id = {issue_id}')\n",
    "            print(f'related_issues = {related_issues}')\n",
    "        \n",
    "        issue_index = find_index_by_issue_id(src_req_csv_file_name, req['id'], verbose)    # int type\n",
    "\n",
    "        #print(f'i = {i}\\n')\n",
    "        #if i == 126:\n",
    "        #    print(f'related issues: type = {type(related_issues)}, value = {related_issues}')\n",
    "        matches = get_dup_ids(related_issues, verbose)    # string type\n",
    "        for match in matches:\n",
    "            match_index = find_index_by_issue_id(src_req_csv_file_name, match, verbose)    # int type\n",
    "            if issue_index == match_index:\n",
    "                print(f'Abnormal: find duplicated info with identical issue id: {issue_id}')\n",
    "                continue\n",
    "            \n",
    "            if issue_index == -1:\n",
    "                print(f'Abnormal: find issue index = -1, for issue id: {issue_id}')\n",
    "                continue\n",
    "\n",
    "            if match_index == -1:\n",
    "                # normal case, the duplicated ticket is not in the test data set\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Find duplicated: issue_index = {issue_index}, match_index = {match_index}')\n",
    "            base[issue_index][match_index] = 1\n",
    "            base[match_index][issue_index] = 1\n",
    "\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81decb27-74d8-4ce9-8c4d-44f80cade1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NUM_OF_REQ = 743 ===\n",
      "\n",
      "Abnormal: find duplicated info with identical issue id: 408\n",
      "Abnormal: find duplicated info with identical issue id: 684\n",
      "Abnormal: find duplicated info with identical issue id: 5005\n",
      "Abnormal: find duplicated info with identical issue id: 5325\n",
      "Abnormal: find duplicated info with identical issue id: 13296\n",
      "Abnormal: find duplicated info with identical issue id: 14341\n",
      "base_dup_count = 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call retrieve_base\n",
    "verbose = False\n",
    "\n",
    "src_req_csv_file_path = 'all_issues_for_test.csv'\n",
    "base = retrieve_base(src_req_csv_file_path, verbose = verbose)\n",
    "\n",
    "# print(f'base = {base}')\n",
    "\n",
    "NUM_OF_REQ = len(base)\n",
    "base_dup_count = 0\n",
    "for i in range(NUM_OF_REQ):\n",
    "    for j in range(i):\n",
    "        if base[i][j] == 1:\n",
    "            base_dup_count += 1\n",
    "            if verbose:\n",
    "                print(f'base[{i}][{j}] = 1\\n')\n",
    "\n",
    "print(f'base_dup_count = {base_dup_count}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194f818-1494-452d-a3f5-d25c35fe1cfa",
   "metadata": {},
   "source": [
    "### 2.3 retrieve the predict array, which stores the duplication of the example datas\n",
    "* predict[i][j] = 1 means: A prompt is sent, and LLM says req[i] and req[j] are duplicated with each other\n",
    "* predict[i][j] = 0 means: A prompt is sent, and LLM says req[i] and req[j] are NOT duplicated with each other\n",
    "* predict[i][j] = -1 means: No prompt is sent, for req[i] and req[j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "086c0000-dc24-429e-a058-19787dd504fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_predict(cosine_similarity_json_file_name, output_json_file_path, verbose = False):\n",
    "    output_data = load_json_in_lines(output_json_file_path)\n",
    "    OUTPUT_LINES = len(output_data)\n",
    "    cos_similarity_data = load_array_from_json(cosine_similarity_json_file_name)\n",
    "    NUM_OF_REQ = len(cos_similarity_data)\n",
    "\n",
    "    print(f'OUTPUT_LINES (total num of prompts Sent) = {OUTPUT_LINES}')\n",
    "    print(f'NUM_OF_REQ (total num of requirements) = {NUM_OF_REQ}')\n",
    "\n",
    "    predict =  [[-1 for _ in range(NUM_OF_REQ)] for _ in range(NUM_OF_REQ)]\n",
    "    line_num = 0\n",
    "    one_count = 0\n",
    "    zero_count = 0\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        for j in range(i):\n",
    "            if cos_similarity_data[i][j] > 0.5:\n",
    "                if line_num < OUTPUT_LINES:    # should always be the case\n",
    "                    data = output_data[line_num]\n",
    "                    content = data['response']['body']['choices'][0]['message']['content']\n",
    "                    match = re.search(r'\\* Probability:\\s*(\\d+)%', content)\n",
    "                    if match:\n",
    "                        probability = int(match.group(1)) / 100.\n",
    "                    \n",
    "                    if verbose:\n",
    "                        #print(f'data = {data}\\n')\n",
    "                        #print(f'content = {content}\\n')\n",
    "                        print(f\"Probability: {probability}\\n\")\n",
    "\n",
    "                    if (probability > 0.7):\n",
    "                        predict[i][j] = predict[j][i] = 1\n",
    "                        one_count += 1\n",
    "                    else:\n",
    "                        predict[i][j] = predict[j][i] = 0\n",
    "                        zero_count += 1\n",
    "\n",
    "                    line_num += 1\n",
    "                    if verbose:\n",
    "                        print(f'predict[{i}][{j}] = {predict[i][j]}\\n')\n",
    "                else:\n",
    "                    print(f'Abornormal case: line_num:{line_num} > OUTPUT_LINES:{OUTPUT_LINES}\\n')\n",
    "                    return\n",
    "\n",
    "    print(f'=== line_num = {line_num}\\n')\n",
    "    print(f'=== one_count of predict = {one_count}\\n')\n",
    "    print(f'=== zero_count of predict = {zero_count}\\n')\n",
    "    \n",
    "    return predict, line_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03883e47-eafe-4e07-b5c6-1615a2fbc000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_LINES (total num of prompts Sent) = 7154\n",
      "NUM_OF_REQ (total num of requirements) = 743\n",
      "=== line_num = 7154\n",
      "\n",
      "=== one_count of predict = 294\n",
      "\n",
      "=== zero_count of predict = 6860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call retrieve_predict\n",
    "cosine_similarity_json_file_name = 'req_cosine_similarity.json'\n",
    "output_json_file_path = 'test/output_files_1/req_batch_output_all.json'\n",
    "\n",
    "predict, num_of_examples = retrieve_predict(cosine_similarity_json_file_name, output_json_file_path, verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d29ce-066f-4e26-b8b9-ce8ca574c78a",
   "metadata": {},
   "source": [
    "## 3. Calculate TP (True Positive), FN (False Negative) and FP (False Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91a155c5-8543-458a-b176-ef958548fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base and predict are 2D arrays, which store the base data and the predicted result\n",
    "# since it's for duplication, base[i][j] = base[j][i], and predict[i][j] = predict[j][i]\n",
    "# and base[i][i] = 1, which we do not care\n",
    "def calc_tp_fn_fp(base, predict, cos_similarity_data, verbose = False):\n",
    "    data_num = len(base)\n",
    "    assert data_num == len(predict) == len(base[0]) == len(predict[0])\n",
    "\n",
    "    print(f'data_num = {data_num}')\n",
    "    \n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    predict_category = [[\"\" for _ in range(data_num)] for _ in range(data_num)]\n",
    "\n",
    "    \n",
    "    for i in range(data_num):\n",
    "        for j in range(i):\n",
    "            if verbose:\n",
    "                print(f'In {inspect.currentframe().f_code.co_name}: base[{i}][{j}] = {base[i][j]}, predict[{i}][{j}] = {predict[i][j]}')\n",
    "            if base[i][j] == 1:\n",
    "                if predict[i][j] == 1:\n",
    "                    tp += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'tp'\n",
    "                elif predict[i][j] == 0:\n",
    "                    fn += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'fn-queried'\n",
    "                elif predict[i][j] == -1:\n",
    "                    fn += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'fn-no-query'\n",
    "                    print(f'For i = {i}, j = {j}, it is a duplicate in base, but we did not query')\n",
    "                    print(f'cos_similarity_data[{i}][{j}] = {cos_similarity_data[i][j]}')\n",
    "                else:\n",
    "                    print(f'Abnormal: base[{i}][{j}]= {base[i][j]}, base[{i}][{j}]= {base[i][j]}, should not happen!!!')\n",
    "                    \n",
    "            elif base[i][j] == 0:\n",
    "                if predict[i][j] == 1:\n",
    "                    fp += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'fp'\n",
    "                elif predict[i][j] == 0:\n",
    "                    tn += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'tn-queried'\n",
    "                elif predict[i][j] == -1:\n",
    "                    tn += 1\n",
    "                    predict_category[i][j] = predict_category[j][i] = 'tn-no-query'\n",
    "                else:\n",
    "                    print(f'Abnormal: base[{i}][{j}]= {base[i][j]}, base[{i}][{j}]= {base[i][j]}, should not happen!!!')\n",
    "            else:\n",
    "                print(f'Abnormal: base[{i}][{j}]= {base[i][j]}, should not happen!!!')\n",
    "        \n",
    "    return tp, fn, fp, tn, predict_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e95f1-d97b-4666-ae5e-182532073a48",
   "metadata": {},
   "source": [
    "## 4. 重建完整信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c80ba70-039b-4d0b-8f7c-7ba03f2ef8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all the records for tp, fn, and fp\n",
    "def save_all_records_for_tp_fn_fp(src_req_csv_file_name, output_json_file_path,\n",
    "                                  cosine_similarity_json_file_name, csv_file_for_analysis,\n",
    "                                  predict_category, include_tn_queried = False, verbose = False):\n",
    "    rows = []\n",
    "\n",
    "    NUM_OF_REQ = len(base)\n",
    "    assert NUM_OF_REQ == len(predict) == len(base[0]) == len(predict[0])\n",
    "    print(f'NUM_OF_REQ = {NUM_OF_REQ}')\n",
    "\n",
    "    df = pd.read_csv(src_req_csv_file_name)\n",
    "    output_data = load_json_in_lines(output_json_file_path)\n",
    "    cos_similarity_data = load_array_from_json(cosine_similarity_json_file_name)\n",
    "    \n",
    "    query_num = 0\n",
    "    for i in range(NUM_OF_REQ):\n",
    "        for j in range(i):\n",
    "            cat = predict_category[i][j]\n",
    "\n",
    "            # If queried, get probability, analysis and new requirement\n",
    "            if (cat == 'tp') or (cat == 'fn-queried') or (cat == 'fp') or (cat == 'tn-queried'):\n",
    "                data = output_data[query_num]\n",
    "                content = data['response']['body']['choices'][0]['message']['content']\n",
    "                if verbose:\n",
    "                    print(f'data = {data}')\n",
    "                    print(f'content = {content}')\n",
    "\n",
    "                probability_match = re.search(r'\\* Probability:\\s*(\\d+)%', content)\n",
    "                if probability_match:\n",
    "                    probability = int(probability_match.group(1)) / 100.\n",
    "                    if verbose:\n",
    "                        print(f'probability = {probability}')\n",
    "\n",
    "                analysis_pattern = r\"\\* Analysis:(.*?)(?=\\*\\sNew Requirement:|\\Z)\"\n",
    "                analysis_match = re.search(analysis_pattern, content, re.DOTALL)\n",
    "                if analysis_match:\n",
    "                    analysis_string = analysis_match.group(1).strip()\n",
    "                    if verbose:\n",
    "                        print(f'analysis_string = {analysis_string}')\n",
    "                \n",
    "                new_requirement_pattern = r\"\\* New Requirement:(.*)\"\n",
    "                new_requirement_match = re.search(new_requirement_pattern, content, re.DOTALL)\n",
    "                if new_requirement_match:\n",
    "                    new_requirement_string = new_requirement_match.group(1).strip()\n",
    "                    if verbose:\n",
    "                        print(f'new_requirement_string = {new_requirement_string}')\n",
    "\n",
    "                # shouldn't happen\n",
    "                if not (probability_match and analysis_pattern and new_requirement_pattern):\n",
    "                    print(f'Abnormal!!! probability_match = {probability_match}')\n",
    "                    print(f'Abnormal!!! analysis_pattern = {analysis_pattern}')\n",
    "                    print(f'Abnormal!!! new_requirement_pattern = {new_requirement_pattern}')\n",
    "                    return\n",
    "\n",
    "                output_query_num = query_num\n",
    "                query_num += 1\n",
    "                output_query_num\n",
    "\n",
    "            elif (cat == 'fn-no-query') or (cat == 'tn-no-query'):\n",
    "                output_query_num = -1\n",
    "                probability = -1\n",
    "                analysis_string = new_requirement_string = \"\"\n",
    "            else:\n",
    "                # shouldn't happen\n",
    "                print(f'Abnormal!!! predict_category[{i}][{j}] = {predict_category[i][j]}')\n",
    "                return\n",
    "\n",
    "            # we are most interested in tp, fp, fn-queried, fn-no-query (by default tn-queried not needed)\n",
    "            if (cat == 'tp') or (cat == 'fp') or (cat == 'fn-queried') or (cat == 'fn-no-query') or ((cat == 'tn-queried') and include_tn_queried):\n",
    "                row = {\n",
    "                    'query num':output_query_num,\n",
    "                    'prediction category':cat,\n",
    "                    'index1':i,\n",
    "                    'issue1 id':df.iloc[i].to_dict()['id'],\n",
    "                    'issue1 subject':df.iloc[i].to_dict()['subject'],\n",
    "                    'issue1 description':df.iloc[i].to_dict()['description'],\n",
    "                    'index2':j,\n",
    "                    'issue2 id':df.iloc[j].to_dict()['id'],\n",
    "                    'issue2 subject':df.iloc[j].to_dict()['subject'],\n",
    "                    'issue2 description':df.iloc[j].to_dict()['description'],\n",
    "                    'consine similarity':cos_similarity_data[i][j],\n",
    "                    'predict probability':probability,\n",
    "                    'predict analysis':analysis_string,\n",
    "                    'new requirement':new_requirement_string\n",
    "                }\n",
    "                rows.append(row)\n",
    "                \n",
    "\n",
    "    save_jsons_array_into_csv(rows, csv_file_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da4b9e-e949-44ec-acc9-5abc69c02f5e",
   "metadata": {},
   "source": [
    "## 5. Main(): Calculate precission, recall and accuracy, and save the result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e0ea016-ae5d-4c5e-be12-8c387c9681c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(base, predict, num_examples):\n",
    "    cosine_similarity_json_file_name = 'req_cosine_similarity.json'\n",
    "    cos_similarity_data = load_array_from_json(cosine_similarity_json_file_name)\n",
    "    \n",
    "    tp, fn, fp, tn, predict_category = calc_tp_fn_fp(base, predict, cos_similarity_data)\n",
    "    print(f'num_examples = {num_examples}')\n",
    "    print(f'tp = {tp}, fn = {fn}, fp = {fp}, tn = {tn}')\n",
    "    assert (tn + tp + fn + fp == num_examples)\n",
    "\n",
    "    if (tp + fn) > 0:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        print(f'tp + fn = {tp + fn}!\\n')\n",
    "\n",
    "    if (tp + fp) > 0:\n",
    "        precission = tp / (tp + fp)\n",
    "    else:\n",
    "        print(f'tp + fp = {tp + fp}!\\n')\n",
    "\n",
    "    if num_examples > 0:\n",
    "        accuracy = (tp + tn) / num_examples\n",
    "    \n",
    "    print(f'Recall: {recall}, Precission: {precission}, Accuracy: {accuracy}')\n",
    "\n",
    "    save_all_records_for_tp_fn_fp('all_issues_for_test.csv', \n",
    "                                  'test/output_files_1/req_batch_output_all.json', \n",
    "                                  'req_cosine_similarity.json',\n",
    "                                  'test/output_files_1/final_for_analysis.csv', \n",
    "                                  predict_category,\n",
    "                                  include_tn_queried = False,\n",
    "                                  verbose = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "007f4ffc-32bd-4f72-a15b-4d22828166d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_num = 743\n",
      "For i = 102, j = 35, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[102][35] = 0.43497796375886916\n",
      "For i = 160, j = 94, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[160][94] = 0.49159859594347655\n",
      "For i = 203, j = 19, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[203][19] = 0.49581302202304617\n",
      "For i = 227, j = 196, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[227][196] = 0.38022486324801\n",
      "For i = 237, j = 154, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[237][154] = 0.4379590898566663\n",
      "For i = 238, j = 191, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[238][191] = 0.3816480808139959\n",
      "For i = 249, j = 145, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[249][145] = 0.4184085183595464\n",
      "For i = 249, j = 182, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[249][182] = 0.469054585424194\n",
      "For i = 249, j = 189, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[249][189] = 0.4918223801206201\n",
      "For i = 254, j = 65, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[254][65] = 0.4937688310088903\n",
      "For i = 281, j = 195, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[281][195] = 0.4584095410934099\n",
      "For i = 322, j = 15, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[322][15] = 0.48577458756396036\n",
      "For i = 323, j = 248, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[323][248] = 0.46550716814371373\n",
      "For i = 432, j = 37, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[432][37] = 0.45674783694004645\n",
      "For i = 456, j = 100, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[456][100] = 0.48697951209598284\n",
      "For i = 492, j = 429, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[492][429] = 0.4772687804507442\n",
      "For i = 537, j = 464, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[537][464] = 0.40956671069986833\n",
      "For i = 622, j = 43, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[622][43] = 0.46071940033214\n",
      "For i = 642, j = 66, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[642][66] = 0.4091700846821395\n",
      "For i = 656, j = 421, it is a duplicate in base, but we did not query\n",
      "cos_similarity_data[656][421] = 0.410505107972224\n",
      "num_examples = 275653\n",
      "tp = 139, fn = 36, fp = 155, tn = 275323\n",
      "Recall: 0.7942857142857143, Precission: 0.47278911564625853, Accuracy: 0.9993070998683127\n",
      "NUM_OF_REQ = 743\n",
      "JSON data has been written to test/output_files_1/final_for_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# call main\n",
    "EXAMPLE_NUM = 743\n",
    "main(base, predict, int(EXAMPLE_NUM*(EXAMPLE_NUM-1)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15084332-8b51-4e37-a828-8f81a0055cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
